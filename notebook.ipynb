{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.52s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pyplot\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision.transforms import Resize\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "import torch\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "coco = COCO('data/annotations/instances_val2017.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['potted plant', 'tv', 'tv', 'chair', 'chair', 'chair', 'chair', 'person', 'person', 'microwave', 'refrigerator', 'book', 'book', 'clock', 'vase', 'vase', 'chair', 'vase', 'vase', 'dining table']\n"
     ]
    }
   ],
   "source": [
    "img = coco.loadImgs(139)\n",
    "annIds = coco.getAnnIds(139)\n",
    "anns = coco.loadAnns(annIds)\n",
    "cat_ids = [ann['category_id'] for ann in anns]\n",
    "# cats = [cat['name'] for cat in cat_ids]\n",
    "cat_objs = coco.loadCats(cat_ids)\n",
    "cats = [cat_obj['name'] for cat_obj in cat_objs]\n",
    "print(cats)\n",
    "mask = np.zeros((img[0]['height'], img[0]['width']))\n",
    "for ann in anns:\n",
    "    mask = np.maximum(coco.annToMask(ann) * ann['category_id'], mask)\n",
    "\n",
    "plt.imshow(mask)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = './data'\n",
    "dataType = 'val2017'\n",
    "\n",
    "# Define the classes (out of the 81) which you want to see. Others will not be shown.\n",
    "filterClasses = ['laptop']\n",
    "\n",
    "# Fetch class IDs only corresponding to the filterClasses\n",
    "catIds = coco.getCatIds(catNms=filterClasses) \n",
    "# Get all images containing the above Category IDs\n",
    "imgIds = coco.getImgIds(catIds=catIds)\n",
    "print(\"Number of images containing all the  classes:\", len(imgIds))\n",
    "\n",
    "img = coco.loadImgs(imgIds[0])[0]\n",
    "I2 = io.imread('{}/images/{}/{}'.format(dataDir,dataType,img['file_name']))/255.0\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(I2)\n",
    "plt.show()\n",
    "annIds = coco.getAnnIds(imgIds=img['id'], catIds=catIds, iscrowd=None)\n",
    "anns = coco.loadAnns(annIds)\n",
    "coco.showAnns(anns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = coco.loadImgs(724)\n",
    "ann_id = coco.getAnnIds(imgIds=[724], iscrowd=None)\n",
    "anns = coco.loadAnns(ann_id)\n",
    "coco.showAnns(anns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_path_list = glob.glob('data/images/val2017/*')\n",
    "val_path_list.sort()\n",
    "trn_path_list = glob.glob('data/images/train2017/*')\n",
    "trn_path_list.sort()\n",
    "\n",
    "trn_img_ids = [int(os.path.split(path)[-1][:12]) for path in trn_path_list]\n",
    "trn_ann_ids = [coco.getAnnIds(img_id) for img_id in trn_img_ids]\n",
    "val_img_ids = [int(os.path.split(path)[-1][:12]) for path in val_path_list]\n",
    "val_ann_ids = [coco.getAnnIds(img_id) for img_id in val_img_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000000139\n",
      "000000000285\n",
      "000000000632\n",
      "000000000724\n",
      "000000000776\n",
      "000000000785\n",
      "000000000802\n",
      "000000000872\n",
      "000000000885\n",
      "000000001000\n"
     ]
    }
   ],
   "source": [
    "for path in val_path_list[:10]:\n",
    "    img_id = os.path.split(path)[-1][:12]\n",
    "    print(img_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "[48114932.34509832 45697304.97647042 41642151.05098017] [66724295.21849895 64859325.40604324 60868457.99031425]\n"
     ]
    }
   ],
   "source": [
    "t_psum = np.array([0.0, 0.0, 0.0], dtype=np.double)\n",
    "t_psum_sq = np.array([0.0, 0.0, 0.0], dtype=np.double)\n",
    "resize = Resize((64, 64))\n",
    "\n",
    "for i, name in enumerate(trn_path_list):\n",
    "    image = Image.open(name)\n",
    "    image = resize(image)\n",
    "    np_data = np.asarray(image)\n",
    "    np_data = np_data / 255\n",
    "    # print(np_data.shape)\n",
    "    if len(np_data.shape) == 3:\n",
    "        t_psum += np.sum(np_data, axis=(0, 1))\n",
    "        t_psum_sq += np.sum(np.sqrt(np_data), axis=(0, 1))\n",
    "    else:\n",
    "        t_psum += np.sum(np_data)\n",
    "        t_psum_sq += np.sum(np.sqrt(np_data))\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "print(t_psum, t_psum_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46987239 0.44626274 0.40666163] [0.43082439 0.43424141 0.42904485]\n"
     ]
    }
   ],
   "source": [
    "count = 25000 * 64 * 64\n",
    "t_mean = t_psum / count\n",
    "t_std = t_psum_sq/count - t_mean**2\n",
    "print(t_mean, t_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "[9614006.80392156 9134584.07450981 8327300.03529412] [13334281.85762674 12969273.83953184 12179058.96998939]\n"
     ]
    }
   ],
   "source": [
    "v_psum = np.array([0.0, 0.0, 0.0], dtype=np.double)\n",
    "v_psum_sq = np.array([0.0, 0.0, 0.0], dtype=np.double)\n",
    "resize = Resize((64, 64))\n",
    "\n",
    "for i, name in enumerate(val_path_list):\n",
    "    image = Image.open(name)\n",
    "    image = resize(image)\n",
    "    np_data = np.asarray(image)\n",
    "    np_data = np_data / 255\n",
    "    if len(np_data.shape) == 3:\n",
    "        v_psum += np.sum(np_data, axis=(0, 1))\n",
    "        v_psum_sq += np.sum(np.sqrt(np_data), axis=(0, 1))\n",
    "    else:\n",
    "        v_psum += np.sum(np_data)\n",
    "        v_psum_sq += np.sum(np.sqrt(np_data))\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "print(v_psum, v_psum_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46943393 0.44602461 0.40660645] [0.43071977 0.43432737 0.42935181]\n"
     ]
    }
   ],
   "source": [
    "count = 5000 * 64 * 64\n",
    "v_mean = v_psum / count\n",
    "v_std = v_psum_sq/count - v_mean**2\n",
    "print(v_mean, v_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 3)\n",
      "<built-in method size of Tensor object at 0x7f81a18d1720>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [640] and output size of [64, 64]. Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m mask_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(mask)\n\u001b[1;32m     19\u001b[0m \u001b[39mprint\u001b[39m(mask_tensor\u001b[39m.\u001b[39msize)\n\u001b[0;32m---> 20\u001b[0m mask_tensor \u001b[39m=\u001b[39m resize(mask_tensor)\n\u001b[1;32m     21\u001b[0m mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(mask_tensor)\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[39mprint\u001b[39m(mask\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/mnist_test/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/mnist_test/lib/python3.9/site-packages/torchvision/transforms/transforms.py:346\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m    339\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mresize(img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mantialias)\n",
      "File \u001b[0;32m~/anaconda3/envs/mnist_test/lib/python3.9/site-packages/torchvision/transforms/functional.py:476\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    473\u001b[0m     pil_interpolation \u001b[39m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m    474\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39mresize(img, size\u001b[39m=\u001b[39moutput_size, interpolation\u001b[39m=\u001b[39mpil_interpolation)\n\u001b[0;32m--> 476\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49mresize(img, size\u001b[39m=\u001b[39;49moutput_size, interpolation\u001b[39m=\u001b[39;49minterpolation\u001b[39m.\u001b[39;49mvalue, antialias\u001b[39m=\u001b[39;49mantialias)\n",
      "File \u001b[0;32m~/anaconda3/envs/mnist_test/lib/python3.9/site-packages/torchvision/transforms/functional_tensor.py:469\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, antialias)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[39m# Define align_corners to avoid warnings\u001b[39;00m\n\u001b[1;32m    467\u001b[0m align_corners \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m interpolation \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mbicubic\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 469\u001b[0m img \u001b[39m=\u001b[39m interpolate(img, size\u001b[39m=\u001b[39;49msize, mode\u001b[39m=\u001b[39;49minterpolation, align_corners\u001b[39m=\u001b[39;49malign_corners, antialias\u001b[39m=\u001b[39;49mantialias)\n\u001b[1;32m    471\u001b[0m \u001b[39mif\u001b[39;00m interpolation \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbicubic\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m out_dtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39muint8:\n\u001b[1;32m    472\u001b[0m     img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mclamp(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39m\u001b[39m255\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/mnist_test/lib/python3.9/site-packages/torch/nn/functional.py:3866\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   3864\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(size, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m   3865\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(size) \u001b[39m!=\u001b[39m dim:\n\u001b[0;32m-> 3866\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3867\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mInput and output must have the same number of spatial dimensions, but got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3868\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minput with spatial dimensions of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m:])\u001b[39m}\u001b[39;00m\u001b[39m and output size of \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3869\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPlease provide input tensor in (N, C, d1, d2, ...,dK) format and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3870\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39moutput size in (o1, o2, ...,oK) format.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3871\u001b[0m \n\u001b[1;32m   3872\u001b[0m         )\n\u001b[1;32m   3873\u001b[0m     output_size \u001b[39m=\u001b[39m size\n\u001b[1;32m   3874\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [640] and output size of [64, 64]. Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format."
     ]
    }
   ],
   "source": [
    "resize = Resize((64, 64))\n",
    "for i, path in enumerate(val_path_list):\n",
    "    image = Image.open(path)\n",
    "    image = resize(image)\n",
    "    np_data = np.asarray(image)\n",
    "    if len(np_data.shape) == 2:\n",
    "        np_data = np.stack([np_data, np_data, np_data], axis=2)\n",
    "    np_data = np.divide(np_data-t_mean, t_std)\n",
    "    print(np_data.shape)\n",
    "\n",
    "    img_id = val_img_ids[i]\n",
    "    img = coco.loadImgs(img_id)\n",
    "    ann_ids = coco.getAnnIds(img_id)\n",
    "    anns = coco.loadAnns(ann_ids)\n",
    "    mask = np.zeros((img[0]['height'], img[0]['width']))\n",
    "    for ann in anns:\n",
    "        mask = np.maximum(coco.annToMask(ann) * ann['category_id'], mask)\n",
    "    mask_tensor = torch.from_numpy(mask)\n",
    "    print(mask_tensor.size)\n",
    "    mask_tensor = resize(mask_tensor)\n",
    "    mask = np.asarray(mask_tensor).astype(int)\n",
    "    print(mask.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_w = 0\n",
    "max_h = 0\n",
    "min_w = 1000\n",
    "min_h = 1000\n",
    "min_ratio = 10\n",
    "max_ratio = 0\n",
    "\n",
    "for i, name in enumerate(val_path_list):\n",
    "    image = Image.open(name)\n",
    "    np_data = np.asarray(image)\n",
    "    h, w = np_data.shape[:2]\n",
    "    min_w = min(min_w, w)\n",
    "    min_h = min(min_h, h)\n",
    "    max_w = max(max_w, w)\n",
    "    max_h = max(max_h, h)\n",
    "\n",
    "    ratio_1 = h / w\n",
    "    ratio_2 = w / h\n",
    "    min_ratio = min(min_ratio, ratio_1, ratio_2)\n",
    "    max_ratio = max(min_ratio, ratio_1, ratio_2)\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "\n",
    "print('max_w:', max_w)\n",
    "print('max_h:', max_h)\n",
    "print('min_w:', min_w)\n",
    "print('min_h:', min_h)\n",
    "print('min_ratio:', min_ratio)\n",
    "print('max_ratio:', max_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_max_w = 0\n",
    "t_max_h = 0\n",
    "t_min_w = 1000\n",
    "t_min_h = 1000\n",
    "t_min_ratio = 10\n",
    "t_max_ratio = 0\n",
    "\n",
    "for i, name in enumerate(trn_path_list):\n",
    "    image = Image.open(name)\n",
    "    np_data = np.asarray(image)\n",
    "    h, w = np_data.shape[:2]\n",
    "    t_min_w = min(t_min_w, w)\n",
    "    t_min_h = min(t_min_h, h)\n",
    "    t_max_w = max(t_max_w, w)\n",
    "    t_max_h = max(t_max_h, h)\n",
    "\n",
    "    ratio_1 = h / w\n",
    "    ratio_2 = w / h\n",
    "    t_min_ratio = min(t_min_ratio, ratio_1, ratio_2)\n",
    "    t_max_ratio = max(t_min_ratio, ratio_1, ratio_2)\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "\n",
    "print('t_max_w:', t_max_w)\n",
    "print('t_max_h:', t_max_h)\n",
    "print('t_min_w:', t_min_w)\n",
    "print('t_min_h:', t_min_h)\n",
    "print('t_min_ratio:', t_min_ratio)\n",
    "print('t_max_ratio:', t_max_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 640, 3)\n",
      "JPEG\n",
      "(640, 426)\n",
      "RGB\n"
     ]
    }
   ],
   "source": [
    "image = Image.open('data/images/val2017/000000000139.jpg')\n",
    "np_data = np.asarray(image)\n",
    "print(np_data.shape)\n",
    "print(image.format)\n",
    "print(image.size)\n",
    "print(image.mode)\n",
    "pyplot.imshow(image)\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c635020d71857411e75f216e7c5ce1f8b739730c5d2238d089ea40f991b51d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
