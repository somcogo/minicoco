{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/somahansel/anaconda3/envs/mnist_test/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pyplot\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision.transforms import Resize\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "import torch\n",
    "from utils.data_loader import get_trn_loader, get_multi_site_trn_loader\n",
    "from models.model import AttUNet\n",
    "\n",
    "# trn_mean = np.asarray([0.46987239, 0.44626274, 0.40666163])\n",
    "# trn_std = np.asarray([0.43082439, 0.43424141, 0.42904485])\n",
    "\n",
    "# val_mean = np.asarray([0.46943393, 0.44602461, 0.40660645])\n",
    "# val_std = np.asarray([0.43071977, 0.43432737, 0.42935181])\n",
    "\n",
    "# coco = COCO('data/annotations/instances_train2017.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = get_trn_loader(batch_size=500)\n",
    "multi_train_dl = get_multi_site_trn_loader(batch_size=200, site_number=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 3, 64, 64])\n",
      "torch.Size([500, 64, 64])\n",
      "torch.Size([500])\n"
     ]
    }
   ],
   "source": [
    "for batch_tuple in train_dl:\n",
    "    img, mask, img_id = batch_tuple\n",
    "    print(img.shape)\n",
    "    print(mask.shape)\n",
    "    print(img_id.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 90, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "model = AttUNet(num_classes=90)\n",
    "out = model(img)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0625684\n",
      "84.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = h5py.File('data/minicoco_trn.hdf5')\n",
    "f2 = h5py.File('data/minicoco_val.hdf5')\n",
    "ds = f2['data']\n",
    "image = np.array(ds[2])\n",
    "print(np.amin(image))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.savefig('image.png')\n",
    "ds2 = f2['mask']\n",
    "mask = np.array(ds2[2])\n",
    "print(np.amax(mask))\n",
    "plt.imshow(mask / 90)\n",
    "plt.axis('off')\n",
    "plt.savefig('mask.png')\n",
    "f.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = coco.loadImgs(139)\n",
    "annIds = coco.getAnnIds(139)\n",
    "anns = coco.loadAnns(annIds)\n",
    "cat_ids = [ann['category_id'] for ann in anns]\n",
    "# cats = [cat['name'] for cat in cat_ids]\n",
    "cat_objs = coco.loadCats(cat_ids)\n",
    "cats = [cat_obj['name'] for cat_obj in cat_objs]\n",
    "print(cats)\n",
    "mask = np.zeros((img[0]['height'], img[0]['width']))\n",
    "for ann in anns:\n",
    "    mask = np.maximum(coco.annToMask(ann) * ann['category_id'], mask)\n",
    "\n",
    "plt.imshow(mask)\n",
    "plt.axis('off')\n",
    "plt.savefig('image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_path_list = glob.glob('data/images/val2017/*')\n",
    "val_path_list.sort()\n",
    "trn_path_list = glob.glob('data/images/train2017/*')\n",
    "trn_path_list.sort()\n",
    "\n",
    "trn_img_ids = [int(os.path.split(path)[-1][:12]) for path in trn_path_list]\n",
    "# trn_ann_ids = [coco.getAnnIds(img_id) for img_id in trn_img_ids]\n",
    "val_img_ids = [int(os.path.split(path)[-1][:12]) for path in val_path_list]\n",
    "# val_ann_ids = [coco.getAnnIds(img_id) for img_id in val_img_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = trn_path_list[0]\n",
    "resize = Resize((64, 64))\n",
    "\n",
    "image = Image.open(path)\n",
    "image = resize(image)\n",
    "np_data = np.asarray(image) / 255\n",
    "np_data = np.divide(np_data - trn_mean, trn_std)\n",
    "print(np.amax(np_data))\n",
    "print(np.amin(np_data))\n",
    "if len(np_data.shape) == 2:\n",
    "    np_data = np.stack([np_data, np_data, np_data], axis=2)\n",
    "\n",
    "img_id = trn_img_ids[0]\n",
    "img = coco.loadImgs(img_id)\n",
    "ann_ids = coco.getAnnIds(img_id)\n",
    "anns = coco.loadAnns(ann_ids)\n",
    "mask = np.zeros((1, 64, 64))\n",
    "for ann in anns:\n",
    "    ann_mask = coco.annToMask(ann) * ann['category_id']\n",
    "    ann_mask = np.expand_dims(ann_mask, axis=0)\n",
    "    ann_mask = torch.Tensor(ann_mask)\n",
    "    ann_mask = np.asarray(resize(ann_mask)).astype(int)\n",
    "    mask = np.maximum(ann_mask, mask)\n",
    "\n",
    "mask = mask.transpose(1, 2, 0)\n",
    "# plt.imshow(image)\n",
    "# plt.axis('off')\n",
    "# plt.savefig('image.png')\n",
    "print(ann_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = './data'\n",
    "dataType = 'val2017'\n",
    "\n",
    "# Define the classes (out of the 81) which you want to see. Others will not be shown.\n",
    "filterClasses = ['laptop']\n",
    "\n",
    "# Fetch class IDs only corresponding to the filterClasses\n",
    "catIds = coco.getCatIds(catNms=filterClasses) \n",
    "# Get all images containing the above Category IDs\n",
    "imgIds = coco.getImgIds(catIds=catIds)\n",
    "print(\"Number of images containing all the  classes:\", len(imgIds))\n",
    "\n",
    "img = coco.loadImgs(imgIds[0])[0]\n",
    "I2 = io.imread('{}/images/{}/{}'.format(dataDir,dataType,img['file_name']))/255.0\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(I2)\n",
    "plt.show()\n",
    "annIds = coco.getAnnIds(imgIds=img['id'], catIds=catIds, iscrowd=None)\n",
    "anns = coco.loadAnns(annIds)\n",
    "coco.showAnns(anns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = coco.loadImgs(724)\n",
    "ann_id = coco.getAnnIds(imgIds=[724], iscrowd=None)\n",
    "anns = coco.loadAnns(ann_id)\n",
    "coco.showAnns(anns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in val_path_list[:10]:\n",
    "    img_id = os.path.split(path)[-1][:12]\n",
    "    print(img_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_psum = np.array([0.0, 0.0, 0.0], dtype=np.double)\n",
    "t_psum_sq = np.array([0.0, 0.0, 0.0], dtype=np.double)\n",
    "resize = Resize((64, 64))\n",
    "\n",
    "for i, name in enumerate(trn_path_list):\n",
    "    image = Image.open(name)\n",
    "    image = resize(image)\n",
    "    np_data = np.asarray(image)\n",
    "    np_data = np_data / 255\n",
    "    # print(np_data.shape)\n",
    "    if len(np_data.shape) == 3:\n",
    "        t_psum += np.sum(np_data, axis=(0, 1))\n",
    "        t_psum_sq += np.sum(np.sqrt(np_data), axis=(0, 1))\n",
    "    else:\n",
    "        t_psum += np.sum(np_data)\n",
    "        t_psum_sq += np.sum(np.sqrt(np_data))\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "print(t_psum, t_psum_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 25000 * 64 * 64\n",
    "t_mean = t_psum / count\n",
    "t_std = t_psum_sq/count - t_mean**2\n",
    "print(t_mean, t_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_psum = np.array([0.0, 0.0, 0.0], dtype=np.double)\n",
    "v_psum_sq = np.array([0.0, 0.0, 0.0], dtype=np.double)\n",
    "resize = Resize((64, 64))\n",
    "\n",
    "for i, name in enumerate(val_path_list):\n",
    "    image = Image.open(name)\n",
    "    image = resize(image)\n",
    "    np_data = np.asarray(image)\n",
    "    np_data = np_data / 255\n",
    "    if len(np_data.shape) == 3:\n",
    "        v_psum += np.sum(np_data, axis=(0, 1))\n",
    "        v_psum_sq += np.sum(np.sqrt(np_data), axis=(0, 1))\n",
    "    else:\n",
    "        v_psum += np.sum(np_data)\n",
    "        v_psum_sq += np.sum(np.sqrt(np_data))\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "print(v_psum, v_psum_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 5000 * 64 * 64\n",
    "v_mean = v_psum / count\n",
    "v_std = v_psum_sq/count - v_mean**2\n",
    "print(v_mean, v_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = Resize((64, 64))\n",
    "for i, path in enumerate(val_path_list):\n",
    "    image = Image.open(path)\n",
    "    image = resize(image)\n",
    "    np_data = np.asarray(image)\n",
    "    if len(np_data.shape) == 2:\n",
    "        np_data = np.stack([np_data, np_data, np_data], axis=2)\n",
    "    np_data = np.divide(np_data-t_mean, t_std)\n",
    "    print(np_data.shape)\n",
    "\n",
    "    img_id = val_img_ids[i]\n",
    "    img = coco.loadImgs(img_id)\n",
    "    ann_ids = coco.getAnnIds(img_id)\n",
    "    anns = coco.loadAnns(ann_ids)\n",
    "    mask = np.zeros((img[0]['height'], img[0]['width']))\n",
    "    for ann in anns:\n",
    "        mask = np.maximum(coco.annToMask(ann) * ann['category_id'], mask)\n",
    "    mask_tensor = torch.from_numpy(mask)\n",
    "    print(mask_tensor.size())\n",
    "    mask_resized = resize(mask_tensor.unsqueeze(0)).squeeze()\n",
    "    mask = np.asarray(mask_tensor).astype(int)\n",
    "    print(mask.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_w = 0\n",
    "max_h = 0\n",
    "min_w = 1000\n",
    "min_h = 1000\n",
    "min_ratio = 10\n",
    "max_ratio = 0\n",
    "\n",
    "for i, name in enumerate(val_path_list):\n",
    "    image = Image.open(name)\n",
    "    np_data = np.asarray(image)\n",
    "    h, w = np_data.shape[:2]\n",
    "    min_w = min(min_w, w)\n",
    "    min_h = min(min_h, h)\n",
    "    max_w = max(max_w, w)\n",
    "    max_h = max(max_h, h)\n",
    "\n",
    "    ratio_1 = h / w\n",
    "    ratio_2 = w / h\n",
    "    min_ratio = min(min_ratio, ratio_1, ratio_2)\n",
    "    max_ratio = max(min_ratio, ratio_1, ratio_2)\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "\n",
    "print('max_w:', max_w)\n",
    "print('max_h:', max_h)\n",
    "print('min_w:', min_w)\n",
    "print('min_h:', min_h)\n",
    "print('min_ratio:', min_ratio)\n",
    "print('max_ratio:', max_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_max_w = 0\n",
    "t_max_h = 0\n",
    "t_min_w = 1000\n",
    "t_min_h = 1000\n",
    "t_min_ratio = 10\n",
    "t_max_ratio = 0\n",
    "\n",
    "for i, name in enumerate(trn_path_list):\n",
    "    image = Image.open(name)\n",
    "    np_data = np.asarray(image)\n",
    "    h, w = np_data.shape[:2]\n",
    "    t_min_w = min(t_min_w, w)\n",
    "    t_min_h = min(t_min_h, h)\n",
    "    t_max_w = max(t_max_w, w)\n",
    "    t_max_h = max(t_max_h, h)\n",
    "\n",
    "    ratio_1 = h / w\n",
    "    ratio_2 = w / h\n",
    "    t_min_ratio = min(t_min_ratio, ratio_1, ratio_2)\n",
    "    t_max_ratio = max(t_min_ratio, ratio_1, ratio_2)\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "\n",
    "print('t_max_w:', t_max_w)\n",
    "print('t_max_h:', t_max_h)\n",
    "print('t_min_w:', t_min_w)\n",
    "print('t_min_h:', t_min_h)\n",
    "print('t_min_ratio:', t_min_ratio)\n",
    "print('t_max_ratio:', t_max_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('data/images/val2017/000000000139.jpg')\n",
    "np_data = np.asarray(image)\n",
    "print(np_data.shape)\n",
    "print(image.format)\n",
    "print(image.size)\n",
    "print(image.mode)\n",
    "pyplot.imshow(image)\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cbed879287a864518ad98a559bd6e1f79516edcf6ebfad5eec1261db191acc0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
